{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Road To ML: Course 4 - Machine Learning with Tree-Based Models in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chap 1: Classification and Regression Trees (CART)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CART**\n",
    "**Advantages**\n",
    "- Learn sequence of if-else quesitions about individual features to infer labels\n",
    "- Easy to understand, use and interpret\n",
    "- Flexibility: Can capture non-linear relationships between features and labels\n",
    "- Don't require scaling i.e. Standardization and Normalization\n",
    "\n",
    "**Limitations**\n",
    "- Classification: Can only produce orthogonal decision boundaries\n",
    "- Sensitive to small variations in the training set\n",
    "- High variance: unconstrained CARTs may overfit the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\\\n",
    "![title](https://i.ibb.co/WkSB752/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decision Boundary/Regions: Linear Model vs CART**\n",
    "Notice: Decision tree divide features space into rectangular regions\\\n",
    "![title](https://i.ibb.co/jD2pXwd/cartt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8859649122807017\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/breast_cancer.csv\",header=0)\n",
    "df['diagnosis'] = df['diagnosis'].replace(['B','M'],[0,1])\n",
    "X = df[['radius_mean','concave points_mean']]\n",
    "y = df['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Score:',accuracy_score(y_test, y_pred))\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper funciton for next exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_labeled_decision_regions(X,y, models):\n",
    "    '''Function producing a scatter plot of the instances contained \n",
    "    in the 2D dataset (X,y) along with the decision \n",
    "    regions of two trained classification models contained in the\n",
    "    list 'models'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas DataFrame corresponding to two numerical features \n",
    "    y: pandas Series corresponding the class labels\n",
    "    models: list containing two trained classifiers \n",
    "    \n",
    "    '''\n",
    "    if len(models) != 2:\n",
    "        raise Exception('''Models should be a list containing only two trained classifiers.''')\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise Exception('''X has to be a pandas DataFrame with two numerical features.''')\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise Exception('''y has to be a pandas Series corresponding to the labels.''')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)\n",
    "    for i, model in enumerate(models):\n",
    "        plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])\n",
    "        ax[i].set_title(model.__class__.__name__)\n",
    "        ax[i].set_xlabel(X.columns[0])\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(X.columns[1])\n",
    "            ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "            ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "    plt.tight_layout()\n",
    "    plt.ylim((0, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIBJIB\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\JIBJIB\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Logistic Regression vs Decision tree\n",
    "\n",
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **How classification tree works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabs**\n",
    "- **Decision Tree**: data structure consisting of a hierachy of nodes.\n",
    "- **Node**: question or prediction.\n",
    "- **Root**: beginning node. no parents, only children nodes.\n",
    "- **Internal node**: node that has a parent, rising 2 children nodes.\n",
    "- **Leaf**: one parent node, no children nodes -> prediction\n",
    "\n",
    "*Tree learn to produce purest leafs*\\\n",
    "![title](https://i.ibb.co/SBpTd55/dictt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain (IG)**\\\n",
    "![title](https://i.ibb.co/4W265yS/IG.png)\n",
    "$$ IG(\\underbrace{f}_{\\text{feature}}, \\underbrace{sp}_{\\text{split-point}} ) = I(\\text{parent}) - \\big( \\frac{N_{\\text{left}}}{N}I(\\text{left}) + \\frac{N_{\\text{right}}}{N}I(\\text{right})  \\big) $$\n",
    "\n",
    "Some criterion used to measure the node's impurity:\n",
    "- gini-index\n",
    "- entropy\n",
    "\n",
    "Classification-tree learning (unconstrained trees):\n",
    "- Nodes are grown recursively (based on the state of its predecessors)\n",
    "- At each node, split the data based on:\n",
    "    -   feature $(f)$ and split-point $(sp)$ to maximize $IG(node)$\n",
    "- If $IG(node) = 0(null)$, the node is declared as a leaf\n",
    "\n",
    "*p.s. for the constrained tree, if the maximum depth is given, all the nodes having a depth of that will be declared as a leaf even if it's not null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following does not follow the rules of unconstrained classification tree?\\\n",
    "A) The existence of a node depends on the state of its predecessors.\\\n",
    "B) The impurity of a node can be determined using different criteria such as entropy and the gini-index.\\\n",
    "C) When the information gain resulting from splitting a node is null, the node is declared as a leaf.\\\n",
    "D) When an internal node is split, the split is performed in such a way so that information gain is minimized.\n",
    "\n",
    "<b>*Answer: D*</b> (splitting an internal node always involves maximizing information gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.9473684210526315\n",
      "Accuracy achieved by using the gini index:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Comparing between the criterion (Gini vs Entropy)\n",
    "df = pd.read_csv(\"datasets/breast_cancer.csv\",header=0)\n",
    "df['diagnosis'] = df['diagnosis'].replace(['B','M'],[0,1])\n",
    "y = df['diagnosis'].values\n",
    "X = df.drop(['diagnosis','id','Unnamed: 32'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=8, random_state=1)\n",
    "\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "y_pred_entropy = dt_entropy.predict(X_test)\n",
    "y_pred_gini = dt_gini.predict(X_test)\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)\n",
    "\n",
    "# Most of the time, the gini index and entropy lead to the same results.\n",
    "# The gini index is slightly faster to compute and is the default criterion -\n",
    "# used in the DecisionTreeClassifier model of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decision-tree for regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Information Criterion for Regression Tree\n",
    "$$ I(\\text{node}) = \\underbrace{\\text{MSE}(\\text{node})}_{\\text{mean-squared-error}} = \\dfrac{1}{N_{\\text{node}}} \\sum_{i \\in \\text{node}} \\big(y^{(i)} - \\hat{y}_{\\text{node}}  \\big)^2 $$\n",
    "$$ \\underbrace{\\hat{y}_{\\text{node}}}_{\\text{mean-target-value}} = \\dfrac{1}{N_{\\text{node}}} \\sum_{i \\in \\text{node}}y^{(i)}$$\n",
    "- Prediction\n",
    "$$ \\hat{y}_{\\text{pred}}(\\text{leaf}) = \\dfrac{1}{N_{\\text{leaf}}} \\sum_{i \\in \\text{leaf}} y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression vs Decision Tree\n",
    "![title](https://i.ibb.co/Fn3LGbB/versus.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training regression tree\n",
    "df = pd.read_csv(\"datasets/auto-mpg.csv\",header=0)\n",
    "col_names = {1 : 'USA', 2:'Europe', 3: 'Asia'}\n",
    "df['origin'] = df['origin'].replace(col_names)\n",
    "df = pd.get_dummies(df, columns =['origin'])\n",
    "\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.drop(['car name',\"cylinders\"],axis=1,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['horsepower']= df['horsepower'].astype('float64')\n",
    "\n",
    "y = df['mpg']\n",
    "X = df.drop('mpg',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=8,min_samples_leaf=0.13,random_state=3)\n",
    "X_train\n",
    "dt.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 3.28\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the regression tree\n",
    "'''The RMSE of a model measures, on average, how much the model's predictions\n",
    "differ from the actual labels. The RMSE of a model can be obtained by computing\n",
    "the square root of the model's Mean Squared Error (MSE).'''\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred_dt)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 3.58\n",
      "Regression Tree test set RMSE: 3.28\n"
     ]
    }
   ],
   "source": [
    "# Linear regression vs regression tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chap 2: The Bias-Variance Tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Generalization Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning: $y = f(x)$, $f$ is unknown\\\n",
    "![title](https://i.ibb.co/znDP0NT/graph-f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals:**\n",
    "- To find $\\hat{f}$ (model) that best approximates $f$ ($\\hat{f} \\approx f$)\n",
    "    - where $\\hat{f}$ is Logistic Regression, Decision Tree, Neural Network, etc..\n",
    "- Discard noise as much as possible\n",
    "- **End Goals:** $\\hat{f}$ achieve low predictive error (high accuracy) on unseen datasets\n",
    "\n",
    "**Difficulties when approximating $f$:**\n",
    "- Overfitting: where the model fits the training set noise\\\n",
    "![title](https://i.ibb.co/ry02sc2/overrr.png)\n",
    "\n",
    "- Underfitting: where the model is not flexible enough to approximate $f$\\\n",
    "![title](https://i.ibb.co/myDCsBr/underr.png)\\\n",
    "*Training set error $\\approx$ testing set error, but the errors are still high*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalization error of $\\hat{f}$:**\n",
    "- Does $\\hat{f}$ perform well on unseen data?\n",
    "- $\\hat{f} = bias^2 + variance + irreducible\\ error$\n",
    "\n",
    "**Bias:**\n",
    "- error term that tells you how much $\\hat{f}$ and $f$ are different ($\\hat{f}\\neq f$)\n",
    "- High bias model -> underfitting\\\n",
    "![title](https://i.ibb.co/WpDWPpk/biasd.png)\\\n",
    "*bias model (black line)*\n",
    "\n",
    "**Variance:**\n",
    "- how much $\\hat{f}$ is inconsistent over diffrent training data sets\n",
    "- High variance model -> overfitting\\\n",
    "![title](https://i.ibb.co/L6hyx7P/varrr.png)\\\n",
    "*variance model (black line)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model complexity**\n",
    "- sets the flexibility of $\\hat{f}$\n",
    "- i.e. increase max tree depth, minimum samples per leaf\n",
    "\n",
    "**Bias-variance tradeoff:**\n",
    "- we need to find the model complexity that achieves the lowest generalization error\n",
    "![title](https://i.ibb.co/NZVWvy0/eoiccc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.ibb.co/WcCwydb/yes.png)\\\n",
    "*center = true function $f$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following correctly describes the relationship between $\\hat{f}$'s complexity and $\\hat{f}$'s bias and variance terms?\\\n",
    "A) As the complexity of $\\hat{f}$ decreases, the bias term decreases while the variance term increases.\\\n",
    "B) As the complexity of $\\hat{f}$ decreases, both the bias and the variance terms increase.\\\n",
    "C) As the complexity of $\\hat{f}$ increases, the bias term increases while the variance term decreases.\\\n",
    "D) As the complexity of $\\hat{f}$ increases, the bias term decreases while the variance term increases.\n",
    "\n",
    "<b>*Answer: D*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following statements is true?\\\n",
    "![title](https://assets.datacamp.com/production/repositories/1796/datasets/f905399bc06da86c2a3af27b20717de5a777e6e1/diagnose-problems.jpg)\n",
    "\n",
    "A) $A$ suffers from high bias and overfits the training set.\\\n",
    "B) $A$ suffers from high variance and underfits the training set.\\\n",
    "C) $B$ suffers from high bias and underfits the training set.\\\n",
    "D) $B$ suffers from high variance and underfits the training set.\n",
    "\n",
    "<b>*Answer: C*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Diagnose bias and variance problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimating Generalization Error**\n",
    "- cannot be done directly since:\n",
    "    - $f$ is unknown\n",
    "    - we usually have 1 dataset\n",
    "    - noise is unpredictable\n",
    "\n",
    "Solution: Split data into training set and test set\n",
    "\n",
    "**Better model evaluation with Cross-Validation**:\n",
    "- test set should not be touched until we are confident with the $\\hat{f}$'s performance\n",
    "- evaluating $\\hat{f}$ on training set gives biased estimate\n",
    "\n",
    "Solution: Cross-validation (K-Fold CV, Hold-Out CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Fold CV**:\n",
    "- the training set is split randomly (*k*) partitions/folds\n",
    "- each time, one fold is picked for evaluation after training $\\hat{f}$ on the other 9 folds\n",
    "- at the end, we will obtain a list of *k* errors\n",
    "    - CV error is computed as the mean of that list\n",
    "\n",
    "**Diagnose Variance Problems**:\n",
    "- $\\hat{f}$ suffer from **high variance** when: CV error > training set error\n",
    "- it **overfit** the training set\n",
    "    - solution: decrease model complecity\n",
    "        - i.e. decrease max depth, increase min samples per leaf, gather more data\n",
    "\n",
    "**Diagnose Bias Problems**:\n",
    "- $\\hat{f}$ suffer from **high bias** when: CV error $\\approx$ training set error $>>$ desired error\n",
    "- it **underfit** the training set\n",
    "    - solution: increase model complecity\n",
    "        - i.e. increase max depth, decrease min samples per leaf, gather more relavent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Evaluate the 10-fold CV error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 4.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Evaluate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 4.09\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 4: High bias or high variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Chapter 1 Lesson 3 (Evaluate the regression tree), we got a $baseline RMSE$ of 3.28 which is considered to be 'good enough'.\n",
    "In the earlier exercise we test it again using $CV$ and $RMSE$ and got a result of 4.26 and 4.09 respectively.\n",
    "\n",
    "it is $CV\\approx RMSE >> baseline RMSE$\\\n",
    "Therefore, our **dt** suffers from high bias and is underfitting the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ensemble Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the limitations of CARTs:\n",
    "- Classification: Can only produce orthogonal decision boundaries\n",
    "- Sensitive to small variations in the training set\n",
    "- High variance: unconstrained CARTs may overfit the training set\n",
    "\n",
    "Ensemble Learning takes the advantage of the flexibility CARTs while reducing their tendency to memorize noise\n",
    "![title](https://i.ibb.co/sj9MT9m/ensemm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voting Classifier**\n",
    "- binary classification task\n",
    "- $N$ classifier making prediction: $P_1, P_2, P_N$ with the output of either 0 or 1\n",
    "- meta model output the final prediction by hard voting. which number has the most amount of vote win!\n",
    "![title](https://i.ibb.co/hBBysJ4/voteme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Define the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=1, max_iter=100, n_jobs=1)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=1)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.764\n",
      "K Nearest Neighbours : 0.724\n",
      "Classification Tree : 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIBJIB\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/indian_liver_patient_preprocessed.csv', header=0)\n",
    "y = df['Liver_disease']\n",
    "X = df.drop('Liver_disease', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state=1)\n",
    "\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIBJIB\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chap 3: Bagging and Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Boostrap aggregation (Bagging)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In bagging, the ensemble is formed by models that use the same training algorithm.\n",
    "    - Theses models are not trained on the entire training set. It is trained on a different subset of the data.\n",
    "- can reduce the variance of individual models in the ensemble.\n",
    "- improve the stability and accuracy of machine learning algorithms\n",
    "- helps to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boostrapping**\n",
    "- will drawn samples with replacement (any samples can be drawn many times)\n",
    "\n",
    "**Aggregating**\n",
    "- consists of drawing N different boostrap samples from training set\n",
    "- each of the boostrap samples are used to train N models that use the same algorithm\n",
    "![title](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging**\\\n",
    "Classification:\n",
    "- aggregates predictions by majority voting\n",
    "- Classifier: BaggingClassifier\n",
    "\n",
    "Regression:\n",
    "- aggregates predictions by averaging\n",
    "-  Regressor: BaggingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Define the bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.66\n",
      "Test set accuracy of dt: 0.63\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/indian_liver_patient_preprocessed.csv', header=0, index_col=0)\n",
    "df.head()\n",
    "y = df['Liver_disease']\n",
    "X = df.drop('Liver_disease', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1,stratify=y)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train,y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test,y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) \n",
    "\n",
    "acc_test_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print('Test set accuracy of dt: {:.2f}'.format(acc_test_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Out of Bag Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging:\n",
    "- some instances may be sampled several times for one model\n",
    "- other instances may not be sampled at all\n",
    "\n",
    "\n",
    "OOB is use to measure the accuracy of classifiers.\n",
    "- For each model, 63% of the training instances are sampled\n",
    "- The remaining 37% constitute OOB instances.\\\n",
    "![title](https://i.ibb.co/vzF8wp4/nuun.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Prepare the ground (OOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.698, OOB accuracy: 0.702\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Random Forests (RF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base estimator: Decision Tree\n",
    "- Each estimator is trained on a di(erent bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "- $d$ features are sampled at each node without replacement\\\n",
    "( $d$ < total number of features )\\\n",
    "![title](https://i.ibb.co/mJ1JR7c/traini.png)\n",
    "![title](https://i.ibb.co/Xb1TSBV/predd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests: Classification & Regression**\\\n",
    "Classification:\n",
    "- aggregates predictions by majority voting\n",
    "- Classifier: RandomForestClassifier\n",
    "\n",
    "Regression:\n",
    "- aggregates predictions by averaging\n",
    "-  Regressor: RandomForestRegressor\n",
    "\n",
    "*in general, Random Forests achieves a lower variacne than individual trees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "- Tree-based methods: enable measuring the importance of each feature in prediction\n",
    "- how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "- use: feature_importance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1: Train an RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=25, random_state=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "\n",
    "df = pd.read_csv('datasets/bike_sharing_demand.csv', header=0)\n",
    "y = df['cnt']\n",
    "X = df.drop('cnt',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 51.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test,y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chap 4: Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backgroud:**\n",
    "- Boosting: Ensemble method combining several weak learners to form a strong learner.\n",
    "- Weak learner do slightly a better job in random guessing\n",
    "    - i.e. Decision stump (CART with max depth of 1)\n",
    "- Boosting train an ensemble of predictors sequentially\n",
    "    - Each predictor tries to correct its predecessor.\n",
    "- Boosting method:\n",
    "    - AdaBoost (adaptive boosting)\n",
    "    - Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaboost (Adaptive Boosting)**\n",
    "- Each predictor pays more attention to the instances that is wrongly predicted by its predecessor by constantly changing the weights of the training instances.\n",
    "- Each predictor assign the coefficient $\\alpha$ which depends on the predictor's training error.\n",
    "![title](https://i.ibb.co/d6dQ3Mc/adada.jpg)\n",
    "\n",
    "**Learning rate**\n",
    "- $0<\\eta<1$\n",
    "- use to shrink the coefficient $\\alpha$ of a trained predictor\n",
    "- small $\\eta$ should be compensate by a great number of estimators\\\n",
    "![title](https://i.ibb.co/zRK3jFG/leagnin.jpg)\n",
    "\n",
    "**Prediction**\n",
    "- Classification:\n",
    "    - weighted majority voting\n",
    "        - AdaBoostClassifier\n",
    "- Regression:\n",
    "    - weighted average\n",
    "        - AdaBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Define the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48766009 0.55418951 0.51921243 0.43926891 0.47732092 0.52519308\n",
      " 0.49235486 0.7124795  0.60950344 0.55051229 0.59791322 0.51055531\n",
      " 0.49465367 0.56453191 0.48368686 0.5929771  0.57332579 0.49180299\n",
      " 0.55586567 0.63671176 0.57658677 0.49592568 0.49968552 0.547432\n",
      " 0.48563625 0.57387496 0.50023951 0.48951048 0.59908865 0.72024892\n",
      " 0.49845777 0.5065494  0.49782885 0.50678095 0.5835487  0.66549909\n",
      " 0.4123509  0.54528444 0.47896774 0.51748472 0.60759261 0.60738142\n",
      " 0.54538028 0.53049161 0.6339782  0.57093478 0.49333084 0.53024335\n",
      " 0.66275964 0.52892576 0.58952004 0.50082124 0.48838653 0.58741738\n",
      " 0.59396335 0.50161923 0.46153584 0.67773    0.47881106 0.59482486\n",
      " 0.5401021  0.47374417 0.50126468 0.69426843 0.50986729 0.57129408\n",
      " 0.48797028 0.54049242 0.52353446 0.52530434 0.64408319 0.5767882\n",
      " 0.49458549 0.48666719 0.45660689 0.56867447 0.49223892 0.49288073\n",
      " 0.48084783 0.48387698 0.5309601  0.50257202 0.50192914 0.50043501\n",
      " 0.55464185 0.50219998 0.49451038 0.48742786 0.53966841 0.63179856\n",
      " 0.47717716 0.52965569 0.58581217 0.55705337 0.50983722 0.50565011\n",
      " 0.51479767 0.5029552  0.47410805 0.52026319 0.58800109 0.56991501\n",
      " 0.62641667 0.54140555 0.49717183 0.49013942 0.49633688 0.72142864\n",
      " 0.70392887 0.51766324 0.48712106 0.46532175 0.58055182 0.60816851\n",
      " 0.48144834 0.56913852]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/indian_liver_patient_preprocessed.csv\", header=0, index_col=0)\n",
    "y = df['Liver_disease']\n",
    "X = df.drop('Liver_disease', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train,y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Gradient Boosting (GB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each predictor in the ensemble corrects its predecessor's error\n",
    "- does not tweak the weights of training instances (unlike AdaBoost)\n",
    "- Fit each predictor is trained using its predecessor's residual errors as labels.\n",
    "- Gradient Boosted Trees: used CART as a base learner\n",
    "![title](https://i.ibb.co/przjM9r/gbtr.jpg)\n",
    "\n",
    "**Shrinkage**\n",
    "- similar to AdaBoost learning rate (eta)\n",
    "- $0<\\eta<1$\n",
    "- use to shrink the prediction $r$\n",
    "- small $\\eta$ should be compensate by a great number of estimators\n",
    "\n",
    "**Prediction**\n",
    "- Regression:\n",
    "    - $y_{pred} = y_1+\\eta r_1+...+\\eta r_N$\n",
    "        - GradientBoostingRegressor\n",
    "- Classification:\n",
    "    - GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Define the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Train the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/bike_sharing_demand.csv\", header = 0)\n",
    "y = df['cnt']\n",
    "X = df.drop('cnt', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 49.537\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_pred, y_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**0.5\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Stochastic Gradient Boosting (SGB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**\n",
    "- Cons of Gradient Boosting\n",
    "    - exhaustive search procedure\n",
    "- each CART is trained to find the best split points and features\n",
    "    - may lead to CARTs using the same split points and maybe the same features\n",
    "\n",
    "solution: Stochastic Gradient Boosting\n",
    "\n",
    "**SGB**\n",
    "- each tree is trained on a random subset of rows of the training data\n",
    "- the sampled instances (40%-80% of the training set) are sampled without replacement\n",
    "- features are sampled without replacement when choosing split points\n",
    "- Results: create diversity, add variances to the ensemble of trees\n",
    "\n",
    "![title](https://i.ibb.co/b5cgX6q/sbggga.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Regression with SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9, # each tree samples 90% of data for training\n",
    "            max_features=0.75, # each tree uses 75% of available features to perform best-split\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Train the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/bike_sharing_demand.csv\", header = 0)\n",
    "y = df['cnt']\n",
    "X = df.drop('cnt', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 47.260\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test,y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chap 5: Model Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tuning a CART's Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**\n",
    "- parameters: learned from data\n",
    "    - CART example: split of a node, split-feature of a node\n",
    "- hyperparameter: not learned from data, set prior to training\n",
    "    - CART example: max_depth, min_samples_leaf, splitting criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning**\n",
    "- **Problem:** search for a set of optimal hyperparameters for a learning algorithm.\n",
    "- **Solution:** find a set of optimal hyperparameters that results in an optimal model.\n",
    "- **Optimal model:** yields an optimal score\n",
    "- **Score:** accuracy (classification) and $R^2$(regression)\n",
    "- cross-validation(CV) is used to estimate the generalization performance.\n",
    "\n",
    "**Why tune hyperparameter?**\n",
    "- default hyperparameter are not optimal for all problems\n",
    "- it should be tune for the best model performance\n",
    "\n",
    "**Hyperparameter: the approach**\n",
    "- Grid search\n",
    "- Random search\n",
    "- Bayesian Optimization\n",
    "- Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Grid search CV**\n",
    "- Manually set a grid of discrete hyperparameter values.\n",
    "- Set a metric for scoring model performance\n",
    "- Search *exhaustively* through the grid.\n",
    "- For each set of hyperparameter, evaluate each model's CV score.\n",
    "- The optimal hyperparameters are those of the model achieving the best CV score.\n",
    "- The bigger the grid, the longer it takes to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Tree hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following is not a hyperparameter of dt (DecisionTreeClassifier)?\\\n",
    "A) min_impurity_decrease\\\n",
    "B) min_weight_fraction_leaf\\\n",
    "C) min_features\\\n",
    "D) splitter \\\n",
    "<b>*Answer: C*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Set the tree's hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params_dt\n",
    "params_dt = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Search for the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini',random_state=1,splitter='best')\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 4: Evaluate the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.707\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "df = pd.read_csv('datasets/indian_liver_patient_preprocessed.csv', header=0)\n",
    "y = df['Liver_disease']\n",
    "X = df.drop('Liver_disease', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state=1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tuning a RF's Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "- Hyperparameter tuning is computational expensive\n",
    "    - sometimes leads to a very slight improvement\n",
    "Better to weight the impact of tuning on the whole project\n",
    "\n",
    "**RF uses:**\n",
    "- CART hyperparameters\n",
    "- number of estimators\n",
    "- boostrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1: Random forests hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following is not a hyperparameter of rf(RandomForestRegressor)?\\\n",
    "A) min_weight_fraction_leaf\\\n",
    "B) criterion\\\n",
    "C) learning_rate\\\n",
    "D) warm_start \\\n",
    "<b>*Answer: C*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2: Set the hyperparameter grid of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 350, 500],\n",
    "    'max_features': ['log2', 'auto', 'sqrt'],\n",
    "    'min_samples_leaf': [2, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3: Search for the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 4: Evaluate the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Test RMSE of best model: 51.116\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import  mean_squared_error as MSE\n",
    "\n",
    "df = pd.read_csv('datasets/bike_sharing_demand.csv', header=0)\n",
    "y = df['cnt']\n",
    "X = df.drop('cnt',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test,y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed0a907aab4b200afcc6c5783b0f4640cec07bc8831b08928d162575f60996d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
